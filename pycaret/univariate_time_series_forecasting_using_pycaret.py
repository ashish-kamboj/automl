# -*- coding: utf-8 -*-
"""time_series_using_pycaret.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pl5DGkhKCcDyGWddyCJ6HUisgroE4dWT
"""

## PyCaret Installation

!pip install --pre pycaret

## Loading data from PyCaret in-built datasets

from pycaret.datasets import get_data
airline = get_data('airline')

## Converting Pandas series to dataframe

df_airline = airline.to_frame().reset_index()
df_airline.head()

## Converting PeriodDtype to timestamp

df_airline['Period'] = df_airline['Period'].dt.to_timestamp('s').dt.strftime('%Y-%m-%d %H:%M:%S.000')
df_airline.head()

## Initializing the training environment and creates the transformation pipeline
## Also split the data into Train and Test

from pycaret.time_series import *
exp_name = setup(data = df_airline,
                 target = "Number of airline passengers", #Specify in case dataframe having more than 1 column
                 index = "Period",
                 ignore_features = None, #Features to be ignored for modeling
                 fold_strategy = "expanding", #Choice of cross validation strategy (expanding/rolling/sliding)
                 fold = 3, #Number of folds to be used in cross validation
                 fh = 12, #forecast horizon
                 seasonal_period = None, #Seasonal period in timeseries data
                 n_jobs = -1, #The number of jobs to run in parallel (for functions that supports parallel processing) -1 means using all processors. To run all functions on single processor set n_jobs to None
                 session_id = 100, #Equivalent to ‘random_state’ in scikit-learn. When None, a pseudo random number is generated, This can be used for later reproducibility of the entire experiment
                 system_log = True,
                 log_experiment = False, #When set to True, all metrics and parameters are logged on the MLflow server
                 experiment_name = None, #Name of the experiment for logging. Ignored when log_experiment is not True
                 log_plots = False, #When set to True, certain plots are logged automatically in the MLFlow server
                 log_profile = False, #When set to True, data profile is logged on the MLflow server as a html file. Ignored when log_experiment is not True
                 log_data = False, #When set to True, dataset is logged on the MLflow server as a csv file. Ignored when log_experiment is not True
                 verbose = True,
                 profile = False #When set to True, an interactive EDA report is displayed
                 )

## Train and evaluate performance of all estimators available in the model library using cross validation.

best_model = compare_models(fold = None, #Controls cross-validation. If None, the CV generator in the 'fold_strategy' parameter of the setup function is used
                            round = 4, #Number of decimal places the metrics in the score grid will be rounded to
                            cross_validation = True, #When set to False, metrics are evaluated on holdout set. 'fold' param is ignored when cross_validation is set to False
                            sort = "SMAPE", #Sort order of the score grid
                            n_select = 1, #Number of top_n models to return
                            verbose = True
                            )

## Train and evaluate the performance of a given estimator using cross validation. Output is a score grid with CV scores by fold. 

#exp_smooth = create_model('exp_smooth')
exp_smooth = create_model(best_model)
exp_smooth

## Tunes the hyperparameters of a given estimator. Output is a score grid with CV scores by fold of the best selected model based on
tuned_exp_smooth = tune_model(exp_smooth)

## Trains a EnsembleForecaster for select models passed in the estimator_list param. Output of this function is a score grid with CV scores by fold.

## Selecting Top3 models for blending(or Ensembling). We can also passed the list of model_ids in compare_models()
top3 = compare_models(n_select = 3, verbose=False)
blender = blend_models(top3)

## Finalizing the model
final_model = best_model

## Analyzes the performance of a trained model on holdout set
## When used without any estimator, this function generates plots on the original data set
## When used with an estimator, it will generate plots on the model residuals

plot_model(plot="diff", data_kwargs={"order_list": [1, 2], "acf": True, "pacf": True})
plot_model(plot="diff", data_kwargs={"lags_list": [[1], [1, 12]], "acf": True, "pacf": True})

plot_model(plot = 'ts')
plot_model(plot = 'decomp', data_kwargs = {'type' : 'multiplicative'})
plot_model(plot = 'decomp', data_kwargs = {'seasonal_period': 24})

plot_model(estimator = final_model, plot = 'forecast', data_kwargs = {'fh' : 24})

## Forecast(For Holdout dataset) using a trained model 

pred_holdout = predict_model(final_model)
pred_holdout

## Training a given estimator on the entire dataset including the holdout set

final_exp_smooth = finalize_model(final_model)

## Forecast(For unseen data) using a trained model(trained on full dataset)

pred_unseen = predict_model(final_exp_smooth, fh = 24)
pred_unseen

## Saving the trained model object into the current working directory as a pickle file for later use

save_model(final_exp_smooth, 'saved_exp_smooth_model')

## Loading a previously saved model

from pycaret.time_series import load_model
saved_exp_smooth = load_model('saved_exp_smooth_model')